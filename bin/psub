#!/bin/bash

# @file psub
# @brief Wrapper for job submission with simplified interface and other options
# to support integration in Portage scripts.
#
# Wrapper for job submissions intended to do, by default, what we usually want,
# and to simplify options we often use.  This script is intended to contain all
# the local peculiarities of the clusters we use.  Other sites using Portage or
# PORTAGEshared may need to adapt this script to their own cluster environment.
# Look for "CUSTOMIZE HERE" below for spots where we expect you might need to
# adapt things.
#
# @author Eric Joanis / Samuel Larkin
#
# Traitement multilingue de textes / Multilingual Text Processing
# Tech. de l'information et des communications / Information and Communications Tech.
# Conseil national de recherches Canada / National Research Council Canada
# Copyright 2006 - 2020, Sa Majeste la Reine du Chef du Canada /
# Copyright 2006 - 2020, Her Majesty in Right of Canada

usage() {
   for msg in "$@"; do
      echo $msg >&2
   done
   cat <<==EOF== >&2

Usage: psub [options] <command line>

  Wrap <command line> into a job script and submit it.
  Caveat: <command line> will be interpreted again in the resulting script,
  so quote or escape any quotes, dollar sign or backslash you need preserved.

Options:

  Cluster options : SLURM / GPSC options:
  -noc(luster)   Block recursive jobs: the job will think it's running on a
                 stand-alone, non-clustered machine.
  -no-slurm      Use SGE scheduler instead of default SLURM
  -i RES_IMAGE   Change the container's image [\$ENV{PSUB_RES_IMAGE}]
  -Q PROJECT     Project's name for fairshare scheduling [\$ENV{PSUB_PROJECT_NAME}]
  -pe ENVIRONMENT Change the parallel environment or Partition (i.e. TrixieMain ) [dev]
  -pepp          Change parallel processes (i.e., number of slots) [1]
  -c CELL        Specify the cell you want your jobs to run in [gpsc1.science.gc.ca]
                 Default can be changed by setting the enviornment variable.
                 export JOBCTL_CELL=gpsc5.science.gc.ca
  -p PRIORITY    SLURM: set the QoS priority; [normal] or low for slurm on GPSC
                 On nodes where we have no allocation , it will be set to low.
                 QSUB:  set the job priority; queued jobs with higher priority
                 should start first. 0 = normal, >0 = high, <0 = low.
                 Only use > 0 for short, urgent jobs. [0]
  Extra SGE Cluster options:
  -l RES=VAL[,RES2=VAL2[,...]]  request specific resources for job
                 (separate multiple resources with commas, or repeat -l)
  -qsparams QSUB_PARAMETERS  additional low-level job submission parameters
                 passed directly to underlying job submission tool
  -t ARRAY       See "qsub -t" in the TORQUE admin manual or man qsub


  General resource options:


  -cpus C        Specify the required number of cpus. [1]
  -mem M         Specify the required amount of memory.  By default, GB are
                 assumed but user can specify G for GB or M for MB. [6G]
  -gpu           Ask for a node with a GPU. Equivalent to -gpus 1
  -gpus G        Ask for G GPUs.  
  -#             shortcut option: # cpus with 6*# GB memory
  -memmap SIZE   request SIZE GB extra virtual memory, to accomodate memory
                 mapped IO.
  -j J           use -j J if you intend to run J processes in the same job, each
                 needing C cpus, M RAM and SIZE GB of shared memory-mapped data.
                 Your job will have J*M+SIZE RAM and J*C cpus, but a ulimit of
                 M+SIZE VMEM and OMP_NUM_THREADS=C for each process. [1]
  -rt RUNTIME    Specify the job's run time in minutes [1440 (one day)]. 
                 When on Trixie the default is 12h (720)  ( max RUNTIME for Trixie ) 
                 The runtime can also use of the following units: s, m, h, d,
                 e.g., -rt 4d will request 4 days of runtime.
                 Default can be changed by setting the enviornment variable
                 PSUB_OVERRIDE_DEFAULT_RUNTIME_MINUTES, e.g. for ten days:
                 export PSUB_OVERRIDE_DEFAULT_RUNTIME_MINUTES=\$((10*24*60))
                 or export PSUB_OVERRIDE_DEFAULT_RUNTIME_MINUTES=10d
  -include-node  Slurm to only use listed nodes
                 ex: -include-node ib12gpu-001 # use only 1 node listed
                 ex: -include-node ib14gpu-[001-004] # use the multi nodes listed
  -exclude-node  Slurm to exclude nodes from being used
                 ex: -exclude-node ib12gpu-001 # exclude 1 nodes
                 ex: -exclude-node  ib14gpu-[001-004] # to exclude multiple nodes


  Note:
  Scripts run by psub can find out the values of the -cpus, -mem, and -memmap
  options in environment variable PSUB_RESOURCE_OPTIONS, while the -j value can
  be found in PSUB_OPT_J.

  Other options:
  -h(elp)        print this help message
  -v(erbose)     print the job script before submitting it
  -N JOB_NAME    specify job name
  -e ERR_FILE    stderr destination
  -o OUT_FILE    stdout destination
  -join          join stderr with stdout in one file
  -noscript      don't write out the script
  -rerun         Job is rerunnable/requeueable when job is preempted [is not]
  -n             "not really" (dump the PBS script, but don't submit it)
  -f             force submission even if errors are detected
  -I             enter interactive mode thus getting a prompt to a node
  -uc            As in usage CPU, keep track of memory and process usage using
                 process-memory-usage.pl.
  -P PERIOD      Log memory usage of process tree every PERIOD minutes. [6]
                 (implies -uc)
  -ug            As in usage GPU, keep track of GPU memory and process usage
                 using nvidia-smi.
  -require       Displays, without executing, how much vmem is required to run
                 the jobs, in MB.
  -require-cpus  Displays, without executing, how many CPUs are required, if
                 specified explicitely; otherwise, has not output.

==EOF==

   exit 1
}

error_exit() {
   echo -n "psub fatal error: " >&2
   for msg in "$@"; do
      echo $msg >&2
   done
   echo "Use -h for help." >&2
   exit 1
}

# arg_check_pos_int $value $arg_name exits with an error if $value does not
# represent a positive integer, using $arg_name to provide a meaningful error
# message.
arg_check_pos_int() {
   expr $1 + 0 &> /dev/null
   RC=$?
   if [ $RC != 0 -a $RC != 1 ] || [ $1 -le 0 ]; then
      error_exit "Invalid argument to $2 option: $1; positive integer expected."
   fi
}

arg_check() {
   if [ $2 -le $1 ]; then
      error_exit "Missing argument to $3 option."
   fi
}

# Convert the resquested memory to MegaBytes for the scheduler.
# Input: number with G or M suffix; no suffix means G
normalizeMemoryRequirement() {
   local MEM_AMOUNT=$1
   if [[ $MEM_AMOUNT ]]; then
      if [[ $MEM_AMOUNT =~ [gG]$ ]]; then
         MEM_AMOUNT=${MEM_AMOUNT%%[gG]};
         MEM_AMOUNT=$(($MEM_AMOUNT * 1024));
      elif [[ $MEM_AMOUNT =~ [mM]$ ]]; then
         MEM_AMOUNT=${MEM_AMOUNT%%[mM]};
      else
         MEM_AMOUNT=$(($MEM_AMOUNT * 1024));
      fi
   fi
   echo $MEM_AMOUNT
}

# Determine what cluster / cluster type we're on
if [[ `on-cluster.sh -type` == jobsub ]]; then
   GPSC=1
else
   # Set your cluster's flag here, to customize to your environment
   true
fi

### GPSC
# Note: a be node has 100000M of ram and 16 cores thus ~6GB of ram per core
VMEM_LIMIT_MB_PER_CPU=6144   # 6 GB

PSUB_CMD=`echo "$0 $*" | perl -pe 'print "# " if $. > 1'`
CPU_COUNT=
VMEM_LIMIT_MB=
MEM_OPTION_MB=
MEMORY_POLLING_DELAY=6
MEMMAP_GB=
PRIORITY=
JOB_ARRAY=
OPT_J=1
NGPUS=
NOCLUSTER_OPTION=
USAGE_CPU=
USAGE_GPU=
USE_SLURM=1
JOIN_OUT_ERR=
PSUB_RESOURCE_OPTIONS=
RESOURCES=
INTMODE=
NOSCRIPT=
ERR_FILE=
OUT_FILE=
QSPARAMS=
NOT_REALLY=
FORCE=
REQUIREMENTS=
CPU_REQUIREMENTS=
# PSUB_RERUN can be set in the environment or on the command line.
export PSUB_RERUN=${PSUB_RERUN}

### GPSC
PARALLEL_ENVIRONMENT=
# NOTE, PE_PARALLEL_PROCESSES should always be 1, unless using MPI or other
# mechanisms that take advantage of multiple slots in one job, i.e., multiple nodes.
PE_PARALLEL_PROCESSES=1

#When on TRIXIE, set a 6h default runtime IF  not set, ELSE 24h is the default
if [[ $CC_CLUSTER == trixie ]]; then
   RUNTIME_SPECS=${PSUB_OVERRIDE_DEFAULT_RUNTIME_MINUTES:-$((12*60))} # default: one day in minutes (24 x 60 = 1440).
else
   RUNTIME_SPECS=${PSUB_OVERRIDE_DEFAULT_RUNTIME_MINUTES:-$((24*60))} # default: one day in minutes (24 x 60 = 1440).
fi


while [ $# -gt 0 ]; do
   case "$1" in
   -h|-help|--help)        usage;;
   -v|-verbose)     VERBOSE=1;;
   -N)              arg_check 1 $# $!;
                    NAME=$2;
                    shift;;
   -[1-9]|-[1-9][0-9])
                    # This is mostly just an message we systematically ignore - but leave it here
                    # commented out for when we work on optimizing frameworks that still use -#.
                    #if [[ $GPSC ]]; then
                    #   echo "Warning: -# is deprecated on gpsc: use -cpus and -mem instead." >&2;
                    #fi
                    CPU_COUNT=$((0 - $1))
                    VMEM_LIMIT_MB=$((VMEM_LIMIT_MB_PER_CPU * $((0 - $1))))
                    MEM_OPTION_MB=
                    [[ $DEBUG_PSUB_RES ]] && echo "$1 => CPU_COUNT=$CPU_COUNT"
                    PSUB_RESOURCE_OPTIONS="$PSUB_RESOURCE_OPTIONS $1";;
   -cpus)           arg_check 1 $# $!
                    CPU_COUNT=$2
                    PSUB_RESOURCE_OPTIONS="$PSUB_RESOURCE_OPTIONS $1 $2"
                    [[ $DEBUG_PSUB_RES ]] && echo "$1 $2 => CPU_COUNT=$CPU_COUNT"
                    shift;;
   -mem)            arg_check 1 $# $!
                    VMEM_LIMIT_MB=`normalizeMemoryRequirement "$2"`
                    PSUB_RESOURCE_OPTIONS="$PSUB_RESOURCE_OPTIONS $1 $2"
                    MEM_OPTION_MB=$VMEM_LIMIT_MB
                    shift;;
   -uc)             USAGE_CPU=1;;
   -ug)             USAGE_GPU=1;;
   -memmap)         arg_check 1 $# $!
                    arg_check_pos_int $2 $1
                    MEMMAP_GB=$2
                    PSUB_RESOURCE_OPTIONS="$PSUB_RESOURCE_OPTIONS $1 $2"
                    shift;;
   -j)              arg_check 1 $# $!
                    arg_check_pos_int $2 $1
                    OPT_J=$2
                    shift;;
   -gpu)            NGPUS=1
                    ;;
   -gpus)           arg_check 1 $# $!
                    arg_check_pos_int $2 $1
                    NGPUS=$2
                    shift;;
   -I)              INTMODE=1;
                    NOSCRIPT=1;
                    [[ $CPU_COUNT ]] || CPU_COUNT=4;;
   -i)              arg_check 1 $# $!;
                    PSUB_RES_IMAGE=$2;
                    shift;;
   -Q)              arg_check 1 $# $!;
                    PSUB_PROJECT_NAME=$2;
                    shift;;
   -pe)             arg_check 1 $# $!;
                    PARALLEL_ENVIRONMENT=$2;
                    shift;;
   -pepp)           arg_check 1 $# $!;
                    PE_PARALLEL_PROCESSES=$2;
                    shift;;
   -rt)             arg_check 1 $# $!;
                    RUNTIME_SPECS=$2;
                    PSUB_RESOURCE_OPTIONS="$PSUB_RESOURCE_OPTIONS $1 $2"
                    shift;;
   -c)              arg_check 1 $# $!;
                    export JOBCTL_CELL=$2;
                    shift;;
   -e)              arg_check 1 $# $!;
                    ERR_FILE=$2;
                    shift;;
   -o)              arg_check 1 $# $!;
                    OUT_FILE=$2;
                    shift;;
   -P)              arg_check 1 $# $!;
                    MEMORY_POLLING_DELAY=$2;
                    USAGE_CPU=1;
                    shift;;
   -noscript)       NOSCRIPT=1;;
   -l)              arg_check 1 $# $!;
                    RESOURCES="${RESOURCES:+$RESOURCES,}$2";
                    shift;;
   -qsparams)       arg_check 1 $# $!;
                    QSPARAMS=$2;
                    shift;;
   -join)           JOIN_OUT_ERR=1;;
   -noc|-nocluster) NOCLUSTER_OPTION="export PORTAGE_NOCLUSTER=1";;
   -p)              arg_check 1 $# $!;
                    PRIORITY="$2";
                    shift;;
   -t)              arg_check 1 $# $!;
                    JOB_ARRAY="$1 $2";
                    shift;;
   -rerun)          PSUB_RERUN=yes;
                    export PSUB_RERUN=yes;;
   -n)              NOT_REALLY=1;;
   -f)              FORCE=1;;
   -require)        REQUIREMENTS=1;;
   -require-cpus)   CPU_REQUIREMENTS=1;;
   -no-slurm)       USE_SLURM=;;
   -exclude-node)   arg_check 1 $# $!;
                    NODE_EXCLUDE=$2;
                    shift;;
   -include-node)   arg_check 1 $# $!;
                    NODE_INCLUDE=$2;
                    shift;;
   --)              shift; break;;
   -*)              error_exit "Unknown options $1.";;
   *)               break;;
   esac
   shift
done

# GPSCC-2 has slurm installed but does not work , forcing SGE in this case
if [[ $JOBCTL_CELL =~ gpscc2 ]]; then
   USE_SLURM=""
fi
# Default is Slurm else it is SGE
if [[ $USE_SLURM == 1 ]]; then
   SCHEDULER=SLURM
else
   SCHEDULER=SGE
fi

if [[ ! $CPU_COUNT ]]; then
   USING_DEFAULT_CPU_COUNT=1
   CPU_COUNT=1
fi

[[ $VMEM_LIMIT_MB ]] || VMEM_LIMIT_MB=$((VMEM_LIMIT_MB_PER_CPU * 1))  # 6GB

COMMAND="$*"

# Output the requirements of memory (in MB) or cpus as a simple count.
function requirements {
   if [[ $REQUIREMENTS ]]; then
      echo $((VMEM_LIMIT_MB + MEMMAP_GB * 1024))
      exit
   elif [[ $CPU_REQUIREMENTS ]]; then
      if [[ ! $USING_DEFAULT_CPU_COUNT ]]; then
         echo $CPU_COUNT
      fi
      exit
   fi
}


function script2Command {
   local PROG_NAME=$1
   local FORCE=$2

   # If the command is a script but doesn't have its x permission bit set,
   # call it with its interpreter
   if [ -f "$PROG_NAME" -a ! -x "$PROG_NAME" -a ! "$FORCE" ]; then
      if LC_ALL=C file $PROG_NAME | grep -q "script text"; then
         # file is a script
         local HASH_BANG_LINE=`head -1 $PROG_NAME`
         local INTERPRETER=${HASH_BANG_LINE/#\#\!}
         INTERPRETER=${INTERPRETER%% *}
         if which-test.sh "$INTERPRETER"; then
            echo Calling your script with "$INTERPRETER"
            COMMAND="$INTERPRETER $COMMAND"
         else
            echo "\"$PROG_NAME\" seems to be a $INTERPRETER script, but \"which\" can't find $INTERPRETER."
            echo "Make your script executable, fix it, or use -f to submit your command as is"
            exit 1
         fi
      else
         echo "\"$PROG_NAME\" is neither executable nor a script."
         echo "Use -f if you really mean to submit your command as is."
         exit 1
      fi
   fi
}


if [[ ! on-cluster.sh && ! $NOT_REALLY ]]; then
   error_exit "We don't seem to be running on a cluster."
fi

# Handle -require and -require-cpus now that all options have been processed
# psub exits in this function if -require or -require-cpus is specified.
requirements

# In interactive mode we need to reserve an entire node
if [[ $INTMODE ]]; then
   if [[ $GPSC ]]; then
      error_exit "-I is not supported (yet?) on the GPSC"
   else
      # CUSTOMIZE HERE if you want to tune psub -I to your own cluster.
      true
   fi
fi

if [[ ! $COMMAND && ! $INTMODE ]]; then
   error_exit "Missing command, nothing to run!"
fi

if [ $INTMODE ]; then
   PROG_NAME=Interactive
else
   # Guess the executable program or script name
   PROG_NAME=`echo "$COMMAND" | perl -e '
      $_ = <>;
      chomp;
      # Use STDIN as the job name if the command seems empty
      $_ or $_ = "STDIN";
      @tokens = split;
      print $tokens[0];
   '`
fi

# The following call modifies COMMAND.
script2Command "$PROG_NAME" "$FORCE"

# Determine the default job name if -N is not specified on the command line.
if [[ ! $NAME ]]; then
   NAME=`basename $PROG_NAME`
fi

if [[ $GPSC ]]; then
   MAX_NAME_LENGTH=30
else
   MAX_NAME_LENGTH=15
fi

# Make sure $NAME is not longer that 15 characters (30 on GPSC), and starts with a letter
# GPSC Note: name length is not limited, but the name must still start with a letter
# Also, / is not allowed in $NAME
NAME=`echo "$NAME" | perl -e "
   \\$_ = <>;
   chomp;
   s#/#_#;
   s/^([^a-z])/J\\$1/i;
   print substr(\\$_, 0, $MAX_NAME_LENGTH);
"`
#printf -v NAME '%q' "$NAME"    # Escape $NAME for reuse as shell input

###############################################################################
###########################   WRITE THE SCRIPT      ###########################
###############################################################################
# This function write the job submission script. It is currently very specific
# to the GPSC, but we hope it should not be too hard to adapt to other clusters.
function write_job_script {
   # PSUB_RES_IMAGE is set in portage/profile.d in order to have
   # control of the image at the project-level.
   local RES_IMAGE=${PSUB_RES_IMAGE:-nrc/nrc_all_default_centos-7-amd64_latest}

   # PSUB_PROJECT_NAME is set in portage/profile.d in order to have
   # control of the project for faireshare at the project-level.
   local PROJECT_NAME=$PSUB_PROJECT_NAME

   TMPSCRIPT=/tmp/psub.`whoami`.`date +%s`.$$
   TMPSCRIPT=`mktemp $TMPSCRIPT.XXX` || error_exit "Cannot create temp file."
   if [[ $USE_SLURM ]]; then
      mv $TMPSCRIPT $TMPSCRIPT.sbatch
      TMPSCRIPT=$TMPSCRIPT.sbatch
   fi
   trap "rm -f $TMPSCRIPT; exit" 0 2 3 13 14 15

   # Calculate the total required value of res_mem
   TOTAL_RES_MEM_MB=$((VMEM_LIMIT_MB * OPT_J + MEMMAP_GB * 1024))
   
   # Even if the PE was specified using -pe, setting the requiremenst needed for the PE
   # This will also add the rules needed for QoS and the other node types on GPSC*.
   # Defaults for GPSC7
   if [[ $USE_SLURM == 1 &&  $JOBCTL_CELL =~ gpsc7 ]]; then
      PRIORITY=low
      if [[ $NGPUS ]]; then
         PARALLEL_ENVIRONMENT=gpu_a100
         PROJECT_NAME_EXTRA=true
      elif [[ $PSUB_DEFAULT_PE ]]; then
         PARALLEL_ENVIRONMENT=$PSUB_DEFAULT_PE
      else
         PARALLEL_ENVIRONMENT=standard
      fi
   # Defaults for GPSC5
   elif [[ $USE_SLURM == 1 && $JOBCTL_CELL =~ gpsc5 ]]; then
      if [[ $PRIORITY -lt 0 ]]; then
         PRIORITY=low
      else
         #New rule on GPSC5, we don't specify the PRIORITY on nodes we have allocations.
         PRIORITY=
      fi
      if [[ $NGPUS ]]; then
         PARALLEL_ENVIRONMENT=gpu_v100
         PROJECT_NAME_EXTRA=true
      elif [[ $PARALLEL_ENVIRONMENT == large ]]; then
         PROJECT_NAME_EXTRA=true
         PRIORITY=low
      elif [[ $TOTAL_RES_MEM_MB -gt 144160 ]]; then
         PARALLEL_ENVIRONMENT=large
         PROJECT_NAME_EXTRA=true
         PRIORITY=low
      elif [[ $PSUB_DEFAULT_PE ]]; then
         PARALLEL_ENVIRONMENT=$PSUB_DEFAULT_PE
      else
         PARALLEL_ENVIRONMENT=standard
      fi
   #New Collab GPSCC-3 (night need to adjust this below onc we know the real settings)
   elif [[ $USE_SLURM == 1 &&  $JOBCTL_CELL =~ gpscc3 ]]; then
         PRIORITY=low
      if [[ $NGPUS ]]; then
         PARALLEL_ENVIRONMENT=gpu_a100
         PROJECT_NAME_EXTRA=true
      elif [[ $PARALLEL_ENVIRONMENT == large ]]; then
         PROJECT_NAME_EXTRA=true
      elif [[ $TOTAL_RES_MEM_MB -gt 512000 ]]; then
         PARALLEL_ENVIRONMENT=large
         PROJECT_NAME_EXTRA=true
      elif [[ $PSUB_DEFAULT_PE ]]; then
         PARALLEL_ENVIRONMENT=$PSUB_DEFAULT_PE
      else
         PARALLEL_ENVIRONMENT=standard
      fi
   # Defaults for Trixie
   elif [[ $USE_SLURM == 1 && $CC_CLUSTER == trixie ]]; then
      PARALLEL_ENVIRONMENT=TrixieMain
   # Defaults for GPSCC-2 ( COLLAB)
   elif [[ $JOBCTL_CELL =~ gpscc2 ]]; then
      if [[ $NGPUS ]]; then
         PARALLEL_ENVIRONMENT=gpu-v100
      elif [[ $TOTAL_RES_MEM_MB -gt 100000 || $CPU_COUNT -gt 16 ]]; then
         PARALLEL_ENVIRONMENT=dev-sln
      else
         PARALLEL_ENVIRONMENT=dev
      fi
   fi
   

   # The job's actual number of CPUs requested is $
   TOTAL_CPU_COUNT=$((OPT_J * CPU_COUNT))

   # Runtime is in seconds.
   local RUNTIME_SECONDS=""
   if [[ $RUNTIME_SPECS =~ ^([0-9]+)s$ ]]; then
      RUNTIME_SECONDS=${BASH_REMATCH[1]}
   elif [[ $RUNTIME_SPECS =~ ^([0-9]+)m$ ]]; then
      RUNTIME_SECONDS=$((${BASH_REMATCH[1]} * 60))
   elif [[ $RUNTIME_SPECS =~ ^([0-9]+)h$ ]]; then
      RUNTIME_SECONDS=$((${BASH_REMATCH[1]} * 60 * 60))
   elif [[ $RUNTIME_SPECS =~ ^([0-9]+)d$ ]]; then
      RUNTIME_SECONDS=$((${BASH_REMATCH[1]} * 24 * 60 * 60))
   elif [[ $RUNTIME_SPECS =~ ^([0-9]+)$ ]]; then
      RUNTIME_SECONDS=$((RUNTIME_SPECS * 60))
   else
      error_exit "Bad -rt value: $RUNTIME_SPECS; must be digits with optional d, h, m or s unit, e.g., 2h."
   fi
   RUNTIME_MINUTES=$((RUNTIME_SECONDS / 60 ))

   local HERE=`pwd`
   ERR_FILE=${ERR_FILE:-$HERE}
   OUT_FILE=${OUT_FILE:-$HERE}

   if [[ $SCHEDULER == SLURM ]]; then
      if [[ -d $ERR_FILE ]]; then ERR_FILE="$ERR_FILE/$NAME.e%j"; fi
      if [[ -d $OUT_FILE ]]; then OUT_FILE="$OUT_FILE/$NAME.o%j"; fi
   fi

   # The CUDA driver allocates a lot of virtual memory.  According to the
   # internet it is something like ram + gpu_cores x some memory.  It is not
   # clear how to calculate how much virtual memory the CUDA driver will need.
   # For K80s with the MPS server, at least 400G extra is needed.
   [[ $NGPUS ]] &&  VMEM_LIMIT_MB=$((400 * 1024 + VMEM_LIMIT_MB))

   # We set the ulimit -v 10MB lower than res_mem because it applies per process, not
   # globally. Those 10MB are for manager scripts and such
   # Known limitation: this won't catch memory problems when the user has multiple memory
   # consuming processes in one job. Those will still most likely get bus errors.
   # Also, the ulimit is not increased by -j, only by -mem and -memmap.
   #
   # For GPU jobs, we simply set VMEM_ULIMIT_KB to unlimited because the GPU
   # requires an excessive amount of VMEM. There are too many influences, such
   # as type of GPU (ram, # cores, etc.), whether the MPS server is running,
   # etc., which makes it difficult to determine an acceptable limit.
   if [[ $NGPUS ]]; then
      VMEM_ULIMIT_KB="unlimited"
   else
      VMEM_ULIMIT_KB=$(( ( VMEM_LIMIT_MB - 10 + MEMMAP_GB * 1024 ) * 1024))
   fi

   if [[ $SCHEDULER == SGE ]]; then
      SUBMIT_CMD="jobsub ${JOBCTL_CELL:+-c $JOBCTL_CELL}"
   else
      SUBMIT_CMD="sbatch"
   fi

if [[ $USE_SLURM ]]; then
   cat <<==EOF== > $TMPSCRIPT
#!/bin/bash
# psub-generated script 
#
#SBATCH --job-name=$NAME
#SBATCH --partition=$PARALLEL_ENVIRONMENT
#${PROJECT_NAME:+SBATCH --account=$PROJECT_NAME${PROJECT_NAME_EXTRA:+__$PARALLEL_ENVIRONMENT}}
#${PRIORITY:+SBATCH --qos=$PRIORITY}
#SBATCH --time=$RUNTIME_MINUTES
#${PSUB_RERUN:+SBATCH --requeue}

#SBATCH --ntasks=$PE_PARALLEL_PROCESSES
#SBATCH --cpus-per-task=$TOTAL_CPU_COUNT
#SBATCH --mem=${TOTAL_RES_MEM_MB}M
#${NGPUS:+SBATCH --gres=gpu:$NGPUS}

#SBATCH --comment="image=$RES_IMAGE"

#SBATCH --output=$OUT_FILE
#SBATCH --error=$ERR_FILE
#SBATCH --open-mode=append
#SBATCH --mail-user==$USER
#SBATCH --mail-type=NONE

#SBATCH --signal=B:15@30

## Priority:
#$ $PRIORITY
## Resources:
#${NODE_INCLUDE:+SBATCH --nodelist=$NODE_INCLUDE}
#${NODE_EXCLUDE:+SBATCH --exclude=$NODE_EXCLUDE}

==EOF==
else
   cat <<==EOF== > $TMPSCRIPT
#!/bin/bash
#
#$ -N "$NAME"
#
# Combines STDERR & STDOUT to STDOUT, if requested
#${JOIN_OUT_ERR:+$ -j y}

## Simply output the jobid.
#$ -terse

## Identifies the ability of a job to be rerun or not.
#$ -r no

#$ -pe $PARALLEL_ENVIRONMENT  $PE_PARALLEL_PROCESSES
#$ -l res_cpus=$TOTAL_CPU_COUNT
#$ -l res_mem=$TOTAL_RES_MEM_MB
#$ -l h_rt=$RUNTIME_SECONDS
#$ -l res_image=$RES_IMAGE
#$ -o $OUT_FILE
#$ -e $ERR_FILE
#$ -S /bin/bash
#$ -M $USER
#$ -notify
#$ ${PROJECT_NAME:+-P $PROJECT_NAME}
#${NGPUS:+$ -l res_gpus=$NGPUS}

## User provided from the command line.
## Job Array:
#$ $JOB_ARRAY
## Priority:
#$ ${PRIORITY:+-p $PRIORITY}
## Resources:
#$ ${RESOURCES:+-l $RESOURCES}
## Additional user-specified job submission parameters
#$ $QSPARAMS

==EOF==
fi

if [[ $SCHEDULER = SLURM ]]; then
   echo "export JOB_ID=\$SLURM_JOB_ID" >> $TMPSCRIPT
fi

cat <<==EOF== >> $TMPSCRIPT
export JOB_NAME="$NAME"

# User's environment.
`env | egrep '^PORTAGE' | sed -e 's/^/export /'`
`env | egrep '^(LD_LIBRARY_PATH|PERL5LIB|PYTHONPATH|MANPATH|TMPDIR|PATH|MADA_HOME)=' | sed -e 's/^/export /'`
`env | egrep '^(LANG|LC_[A-Z]+)=' | sed -e 's/^/export /'`
`env | egrep '^(PSUB_DEFAULT_PE|PSUB_NO_DEBUG|PSUB_OVERRIDE_DEFAULT_RUNTIME_MINUTES|PSUB_RERUN|JOBCTL_CELL)=' | sed -e 's/^/export /'`
$NOCLUSTER_OPTION

# Export PSUB_RES_IMAGE so that children will use the same OS res_image.
# This is particularly important if the -i flag was used to specify an alternate image.
export PSUB_RES_IMAGE=$RES_IMAGE

# Set the number of allowed openmp threads based on the number of cpus.
export OMP_NUM_THREADS=$CPU_COUNT

# Tell children what resources this job was submitted with
export PSUB_RESOURCE_OPTIONS="$PSUB_RESOURCE_OPTIONS"
export PSUB_OPT_J=$OPT_J

# TODO: redefine TMPDIR to use SSDs.

# TODO: we shouldn't source portage's profile !!?
#source /space/project/portage/main/profile

# On the GPSC, we set a ulimit -v to the memory requested, because otherwise the process
# just dies with a bus error when it exceeds its memory allocation.
ulimit -v $VMEM_ULIMIT_KB

######################
# DEBUGGING
if [[ ! \$PSUB_NO_DEBUG ]]; then
   echo DEBUGGING
   ( set -o posix ; set )
   whoami
   id
   pwd
   [[ -d /space/project/portage ]] && ls -l /space/project/portage
   [[ -d /space/group/nrc_ict/pkgs ]] && ls -l /space/group/nrc_ict/pkgs
   echo
   lsb_release -a
   lscpu
   head -25 /proc/cpuinfo | grep "model name"
   echo END DEBUGGING
   echo; echo; echo; echo
fi >&2

# GPU
if [[ -n "$NGPUS" ]]; then
   hostname;
   uname -a;
   # What GPU id is assigned to this job.
   grep CUDA /etc/rc.local;
   eval \$(grep CUDA /etc/rc.local)
   #nvidia-smi -L
   nvidia-smi -L | sed -E '/'\$(tr "," "\|" <<< \$CUDA_VISIBLE_DEVICES)'/ s/^/   /'
   #nvidia-smi -i \$CUDA_VISIBLE_DEVICES
   nvidia-smi
   pgrep -l nvidia;
fi | sed 's/^/   /' >&2
# DEBUGGING

# Preserve the calling working directory
cd $HERE || { ! echo "Error: Cannot change directory to $HERE" >&2; exit; }

if [[ -n "$USAGE_CPU" ]]; then
   process-memory-usage.pl -s 3 $(($MEMORY_POLLING_DELAY*60)) \$\$ &> "$NAME".c\$JOB_ID &
   readonly pmu_pid=\$!
fi

if [[ -n "$NGPUS" && -n "$USAGE_GPU" ]]; then
   # Humm looks like we need to re-eval to get CUDA_VISIBLE_DEVICES?!
   eval \$(grep CUDA /etc/rc.local)
   echo "CUDA_VISIBLE_DEVICES: \$CUDA_VISIBLE_DEVICES" >&2
   nvidia-smi \
      --id=\$CUDA_VISIBLE_DEVICES \
      --loop=10 \
      --query-gpu=utilization.gpu,utilization.memory,memory.used,memory.free,memory.total,driver_version \
      --format=csv \
      &> "$NAME".g\$JOB_ID &
   readonly nvidia_pid=\$!
fi

# Trap kill signals
trap 'echo "[\`date\` \`hostname\`] psub-generated script \`basename \$0\` caught SIGINT, aborting." >&2; kill -15 "\$CMD_PID"; kill -15 -"\$CMD_PID"; sleep 15; wait "\$CMD_PID"; exit 2' 2
trap 'echo "[\`date\` \`hostname\`] psub-generated script \`basename \$0\` caught SIGQUIT, aborting." >&2; kill -3 "\$CMD_PID"; kill -3 -"\$CMD_PID"; sleep 15; wait "\$CMD_PID"; exit 3' 3
trap 'echo "[\`date\` \`hostname\`] psub-generated script \`basename \$0\` caught SIGUSR1, aborting." >&2; kill -10 "\$CMD_PID"; kill -10 -"\$CMD_PID"; sleep 15; wait "\$CMD_PID"; exit 10' 10
trap 'echo "[\`date\` \`hostname\`] psub-generated script \`basename \$0\` caught SIGUSR2, aborting." >&2; kill -12 "\$CMD_PID"; kill -12 -"\$CMD_PID"; sleep 15; wait "\$CMD_PID"; exit 12' 12
trap 'echo "[\`date\` \`hostname\`] psub-generated script \`basename \$0\` caught SIGTERM, aborting." >&2; kill -15 "\$CMD_PID"; kill -15 -"\$CMD_PID"; sleep 15; wait "\$CMD_PID"; exit 15' 15

# Display the exact start time
echo ============ Starting job \$JOB_ID on \`date\` on node \`hostname\` OS \`cat /etc/*release | grep PRETTY_NAME | grep -o '".*"'\` >&2

# set -m makes it so that the command PID is also the PGID, so we can kill all
# its child processes at once, with kill -N -PGID in the traps above.
set -m

# User command, in the background so we can catch signals on Slurm too, and in a subshell
# because the user command is allowed to be a whole one-liner script in itself.
( $COMMAND ) &
CMD_PID="\$!"
wait "\$CMD_PID"
RC=\$?

# Display the exact end time
echo ============ Finished job \$JOB_ID on \`date\` with rc=\$RC >&2

# Stop monitoring
[[ -n "\$pmu_pid\$nvidia_pid" ]] && kill -9 \$pmu_pid \$nvidia_pid

# Display resources used on stderr when the user command has completed}
echo scheduler = $SCHEDULER  >&2
if [[ $SCHEDULER = SLURM ]]; then
   echo scontrol show jobid -dd \$JOB_ID >&2
   scontrol show jobid -dd \$JOB_ID >&2
else
   echo jobst -f -j \$JOB_ID >&2
   jobst -f -j \$JOB_ID >&2
fi

# Command executed
# $SUBMIT_CMD $TMPSCRIPT

# original psub command given by user (with quotes and \\ removed)
# $PSUB_CMD
==EOF==
}


write_job_script

# On some clusters, the script has to be executable
chmod +x $TMPSCRIPT

if [[ $VERBOSE ]]; then
   cat $TMPSCRIPT >&2
   echo "" >&2
fi

if [[ $NOT_REALLY ]]; then
   ls -l $TMPSCRIPT >&2
   cat $TMPSCRIPT
   echo ""
   exit
fi

START=`date`
# Note: we explicitly delete the warning about -N being set twice, because we really, really don't care!
JOBID=`$SUBMIT_CMD $TMPSCRIPT | grep -v '^warning: "-N" option has already been set, overriding previous setting$'`
QSUB_RC=$?
SAVE_JOBID=$JOBID
# To work around issues with jobsub directing error messages to stdout, we
# assume the last line of output is our jobname and strip preceding text.
JOBID=$(cat <<<"$JOBID" | tail -1)
if [[ $JOBID =~ ([0-9]+) ]]; then
    JOBID=${BASH_REMATCH[1]}
else
    JOBID=Unknown
fi
[[ $SAVE_JOBID != $JOBID ]] && echo $SAVE_JOBID >&2
echo $JOBID

if [[ ! $NOSCRIPT ]]; then
   JOBSCRIPT="$NAME".j${JOBID:-0}
   cp "$TMPSCRIPT" "$JOBSCRIPT"
   # mktemp doesn't give any group permissions, so we need to set appropriate
   # group read/execute permissions on the jobfile: union of its original perms
   # and the group read/execute perms of the current directory.
   chmod $(printf %o $(( 0$(stat -c %a .) & 0050 | 0$(stat -c %a "$JOBSCRIPT") )) ) "$JOBSCRIPT"
fi

if [[ $QSUB_RC != 0 ]]; then
   if [[ $USE_SLURM ]]; then
      error_exit "Job submission failed (resources requested might not exist, look for a slurm error message above):" "SUBMIT_CMD $TMPSCRIPT"
   else
      error_exit "Job submission failed:" "$SUBMIT_CMD $TMPSCRIPT"
   fi
fi

exit 0
